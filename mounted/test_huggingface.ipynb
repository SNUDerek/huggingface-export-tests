{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34123c5f",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- (torch) https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html\n",
    "- (torchscript) https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\n",
    "- (onnx export) https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n",
    "- (onnx export example) https://colab.research.google.com/github/PyThaiNLP/tutorials/blob/master/source/notebooks/thai_wav2vec2_onnx.ipynb#scrollTo=5-lBGrVVj0i_\n",
    "- (onnx FP16) https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/converter_scripts/float32_float16_onnx.ipynb\n",
    "- (onnxruntime) https://onnxruntime.ai/docs/get-started/with-python.html\n",
    "- (tensorrt saving, inference) https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/\n",
    "- (tensorrt inference, fixed) https://stackoverflow.com/questions/59280745/inference-with-tensorrt-engine-file-on-python\n",
    "- (torch-tensorrt) https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/\n",
    "- (torch-tensorrt python) https://nvidia.github.io/Torch-TensorRT/tutorials/getting_started_with_python_api.html\n",
    "- model info: https://huggingface.co/facebook/wav2vec2-base-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10cc88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxmltools\n",
    "import onnxruntime\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit \n",
    "import time\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import torch.onnx\n",
    "import torch_tensorrt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchaudio.models.wav2vec2.utils import import_huggingface_model\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from pynvml import *\n",
    "nvmlInit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf1897-d863-406b-a5ba-813f633055d4",
   "metadata": {},
   "source": [
    "### check import versions, cuda support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7e91f1-bf6b-4f97-9923-bd49dabd271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvinfer-bin                  8.2.1-1+cuda11.4                  amd64        TensorRT binaries\n",
      "ii  libnvinfer-dev                  8.2.1-1+cuda11.4                  amd64        TensorRT development libraries and headers\n",
      "ii  libnvinfer-plugin-dev           8.2.1-1+cuda11.4                  amd64        TensorRT plugin libraries and headers\n",
      "ii  libnvinfer-plugin8              8.2.1-1+cuda11.4                  amd64        TensorRT plugin library\n",
      "ii  libnvinfer8                     8.2.1-1+cuda11.4                  amd64        TensorRT runtime libraries\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfdcdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.10.1+cu113\n",
      "pytorch cuda?  : True\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch version:\", torch.__version__)\n",
    "print(\"pytorch cuda?  :\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6addd82a-b124-4055-869e-41d58ffa7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime version: 1.9.0\n",
      "onnxruntime device : GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"onnxruntime version:\", onnxruntime.__version__)\n",
    "print(\"onnxruntime device :\", onnxruntime.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63834519-cd94-4e65-b2a3-09343a8ed1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 stats:\n",
      "total    : 11283904.00 MB\n",
      "free     : 11123264.00 MB\n",
      "used     :   160640.00 MB\n"
     ]
    }
   ],
   "source": [
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(\"GPU 0 stats:\")\n",
    "print(f'total    : {info.total/1024.:>11.2f} MB')\n",
    "print(f'free     : {info.free/1024.:>11.2f} MB')\n",
    "print(f'used     : {info.used/1024.:>11.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e9c25-5152-4540-b6d7-5844345ba5a4",
   "metadata": {},
   "source": [
    "## export HuggingFace to PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3ebdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# (down)load model and tokenizer\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "hugging_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3122823e-034b-43b7-8687-69eaa12b01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import as native pytorch model\n",
    "imported_model = import_huggingface_model(hugging_model)\n",
    "_ = imported_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d806022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input\n",
    "AUDIO_MAXLEN = 100000\n",
    "dummy_input = torch.randn(1, AUDIO_MAXLEN, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59624955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2359: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n"
     ]
    }
   ],
   "source": [
    "# export with native torch onnx\n",
    "torch.onnx.export(imported_model,     # model being run \n",
    "     dummy_input,                     # model input (or a tuple for multiple inputs) \n",
    "     \"wav2vec2-ctc.onnx\",             # where to save the model  \n",
    "     export_params=True,              # store the trained parameter weights inside the model file \n",
    "     opset_version=14,                # the ONNX version to export the model to \n",
    "     do_constant_folding=True,        # whether to execute constant folding for optimization \n",
    "     input_names = ['modelInput'],    # the model's input names \n",
    "     output_names = ['modelOutput'],  # the model's output names \n",
    "     dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                   'modelOutput' : {0 : 'batch_size'}}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c733167-d871-4a2a-bdf4-ccb66f224719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted model to fp16 in: 1661.193 ms\n"
     ]
    }
   ],
   "source": [
    "# convert to fp16\n",
    "s = time.perf_counter()\n",
    "onnx_model = onnxmltools.utils.load_model(\"wav2vec2-ctc.onnx\")\n",
    "onnx_model = onnxmltools.utils.float16_converter.convert_float_to_float16(onnx_model)\n",
    "onnxmltools.utils.save_model(onnx_model, \"wav2vec2-ctc.fp16.onnx\")\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "print(f\"converted model to fp16 in: {(e-s):.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e76124-cc39-4e75-aae4-3f39cd3859f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (not working on wav2vec) export PyTorch to TRT with torch_tensorrt\n",
    "\n",
    "torch_tensorrt (TRTorch) uses TorchScript as an intermediate format, so test conversion with `torch.jit.script()` and `torch.jit.trace()`.\n",
    "\n",
    "then, demonstrate working conversion on a small test model.\n",
    "\n",
    "however, kernel dies (segfault) when converting actual model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2396e969-7e81-4a1c-aeac-d66921afe467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing weight_norm from ConvolutionalPositionalEmbedding\n"
     ]
    }
   ],
   "source": [
    "# scripting works\n",
    "scripted_model = torch.jit.script(imported_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813b99cd-831f-46b4-9258-59f5bea9ebb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: RuntimeError: Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n"
     ]
    }
   ],
   "source": [
    "# tracing raises an error\n",
    "try:\n",
    "    traced_model = torch.jit.trace(imported_model, dummy_input, strict=False)\n",
    "except Exception as e:\n",
    "    print(f\"error: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fddc670e-9394-4ef3-9486-1c3484b32f93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Cannot infer input type from calcuations in graph for input input.1. Assuming it is Float32. If not, specify input type explicity\n"
     ]
    }
   ],
   "source": [
    "test_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.Linear(100, 100),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "trt_test_model = torch_tensorrt.compile(test_model, \n",
    "    inputs= [torch_tensorrt.Input((1, 100))],\n",
    "    enabled_precisions= {torch_tensorrt.dtype.float, torch_tensorrt.dtype.half} # Run with FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a0de3fc-f788-4541-be79-ec1b54c5a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS WILL FAIL...\n",
    "# trt_model = torch_tensorrt.compile(imported_model, \n",
    "#     inputs= [torch_tensorrt.Input((1, AUDIO_MAXLEN))],\n",
    "#     enabled_precisions= {torch_tensorrt.dtype.float, torch_tensorrt.dtype.half} # Run with FP16\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ac54d-cf9b-433a-ae4a-4391c7ecb150",
   "metadata": {
    "tags": []
   },
   "source": [
    "### export ONNX to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3811330a-869e-4bbe-8bf8-950b2327a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32885630-80c5-48aa-a36b-699845b28268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export helper functions for tensorRT\n",
    "# adapted for wav2vec from https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/\n",
    "\n",
    "def build_engine(onnx_path, shape):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "        builder.create_network(1) as network, \\\n",
    "        builder.create_builder_config() as config, \\\n",
    "        trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        config.max_workspace_size = (256 << 20)\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            parser.parse(model.read())\n",
    "            network.get_input(0).shape = shape\n",
    "            engine = builder.build_engine(network, config)\n",
    "            return engine\n",
    "\n",
    "def save_engine(engine, file_name):\n",
    "    buf = engine.serialize()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(buf)\n",
    "        \n",
    "def load_engine(trt_runtime, plan_path):\n",
    "    with open(plan_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07c00f5-f870-47bd-8fe4-40054b9d4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/14/2022-13:45:54] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_411/44071790.py:13: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for trt export: 32.976 s\n",
      "model exported as  : wav2vec2-ctc.trt\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "\n",
    "onnx_file_name = 'wav2vec2-ctc.onnx'\n",
    "tensorrt_file_name = 'wav2vec2-ctc.trt'\n",
    "input_shape = [1, AUDIO_MAXLEN]\n",
    "\n",
    "engine = build_engine(onnx_file_name, input_shape)\n",
    "save_engine(engine, tensorrt_file_name) \n",
    "\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "print(f\"time for trt export: {e-s:.3f} s\")\n",
    "print(f\"model exported as  : {tensorrt_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061f10e6-76ae-4f12-8cb3-dee0e5480217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_411/44071790.py:13: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for trt export: 26.772 s\n",
      "model exported as wav2vec2-ctc.fp16.trt\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "\n",
    "onnx_fp16_file_name = 'wav2vec2-ctc.fp16.onnx'\n",
    "tensorrt_fp16_file_name = 'wav2vec2-ctc.fp16.trt'\n",
    "input_shape = [1, AUDIO_MAXLEN]\n",
    "\n",
    "engine = build_engine(onnx_fp16_file_name, input_shape)\n",
    "save_engine(engine, tensorrt_fp16_file_name) \n",
    "\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "print(f\"time for trt export: {e-s:.3f} s\")\n",
    "print(f\"model exported as {tensorrt_fp16_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d67724-7a0d-4b9a-bec4-aacec0a9ea12",
   "metadata": {},
   "source": [
    "# ======================== INFERENCE ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd0ac3-036a-44a0-b21e-8e9cfd8550ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test PyTorch vs ONNX inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e15a548-c315-4a41-a535-8eb45ee2ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iters = 256\n",
    "speech_input = np.random.random(size=(1, AUDIO_MAXLEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dde6cc-f61d-4ce6-8221-7f9151722771",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48a4d8c7-ef1e-4d9c-be00-7270a7ea1330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "gpu_model = imported_model\n",
    "gpu_model.to(\"cuda\")\n",
    "print(next(gpu_model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1bf0b9f4-17e0-429a-96fc-2aa578b71371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "speech = torch.from_numpy(np.copy(speech_input)).float().to(\"cuda\")\n",
    "torch_prediction = gpu_model(speech)[0].detach().cpu().numpy()\n",
    "print(torch_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8de0230-a631-4530-9f5f-9da75e772166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 256/256 [00:03<00:00, 66.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for torch GPU inference: 15.128 ms per inference\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm.trange(test_iters):\n",
    "        prediction = gpu_model(speech)\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "torch_inference = e - s\n",
    "print(f\"time for torch GPU inference: {torch_inference*1000./test_iters:.3f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9181821-fa34-4058-9c94-3298729e4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377844224\n"
     ]
    }
   ],
   "source": [
    "del prediction\n",
    "del speech\n",
    "del gpu_model\n",
    "time.sleep(1)\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f7883517-9a60-4bc7-a763-6ad97e03fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 stats:\n",
      "total    : 11283904.00 MB\n",
      "free     :  6576704.00 MB\n",
      "used     :  4707200.00 MB\n"
     ]
    }
   ],
   "source": [
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(\"GPU 0 stats:\")\n",
    "print(f'total    : {info.total/1024.:>11.2f} MB')\n",
    "print(f'free     : {info.free/1024.:>11.2f} MB')\n",
    "print(f'used     : {info.used/1024.:>11.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f13147-37b8-4eda-b1a1-e172e6a68314",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "852d68a1-3958-4e1a-9ba0-80b0898d4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 'wav2vec2-ctc.onnx'\n"
     ]
    }
   ],
   "source": [
    "print(f\"loading model '{onnx_file_name}'\")\n",
    "ort_model = onnxruntime.InferenceSession(onnx_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1e843ee-0d65-4de4-80c7-003bf10152c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "speech = np.copy(speech_input).astype(np.float32)\n",
    "ort_inputs = {\"modelInput\": speech}\n",
    "ort_outs = ort_model.run(None, ort_inputs)\n",
    "ort32_prediction = np.array(ort_outs)[0]\n",
    "print(ort32_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14383929-2b4f-47fc-82cb-d53930b571a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 256/256 [00:03<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for onnx-runtime fp32 GPU inference: 14.541 ms per inference\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "for _ in tqdm.trange(test_iters):\n",
    "    ort_outs = ort_model.run(None, ort_inputs)\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "ort32_inference = e - s\n",
    "print(f\"time for onnx-runtime fp32 GPU inference: {ort32_inference*1000./test_iters:.3f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f805ca2-05e9-41a4-9560-518354e0ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ort_model\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb399b38-4dab-48e9-a67c-6b67718f0a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 stats:\n",
      "total    : 11283904.00 MB\n",
      "free     :  6576704.00 MB\n",
      "used     :  4707200.00 MB\n"
     ]
    }
   ],
   "source": [
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(\"GPU 0 stats:\")\n",
    "print(f'total    : {info.total/1024.:>11.2f} MB')\n",
    "print(f'free     : {info.free/1024.:>11.2f} MB')\n",
    "print(f'used     : {info.used/1024.:>11.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099b30e-6bea-4c12-b02d-5f04fde83fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ONNX FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31cac2c7-a2a9-4375-9ef6-ef036b68476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 'wav2vec2-ctc.fp16.onnx'\n"
     ]
    }
   ],
   "source": [
    "print(f\"loading model '{onnx_fp16_file_name}'\")\n",
    "ort16_model = onnxruntime.InferenceSession(onnx_fp16_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c71e5ea4-e8b2-4fb8-aeac-61550eb3a02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "speech16 = np.copy(speech_input).astype(np.half)\n",
    "ort16_inputs = {\"modelInput\": speech16}\n",
    "ort16_outs = ort16_model.run(None, ort16_inputs)\n",
    "ort16_prediction = np.array(ort16_outs[0])\n",
    "print(ort16_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f41919e-9173-4006-b20b-85a7a40f2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 256/256 [00:01<00:00, 129.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for onnx-runtime fp16 GPU inference: 7.733 ms per inference\n"
     ]
    }
   ],
   "source": [
    "ort16_inputs = {\"modelInput\": speech16}\n",
    "s = time.perf_counter()\n",
    "for _ in tqdm.trange(test_iters):\n",
    "    ort_outs = ort16_model.run(None, ort16_inputs)\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "ort16_inference = e - s\n",
    "print(f\"time for onnx-runtime fp16 GPU inference: {ort16_inference*1000./test_iters:.3f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cac3de6-61c7-4645-a424-8f97b575d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ort16_model\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56763e1b-9ae0-4a58-9911-3384134e0240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 stats:\n",
      "total    : 11283904.00 MB\n",
      "free     :  6576704.00 MB\n",
      "used     :  4707200.00 MB\n"
     ]
    }
   ],
   "source": [
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(\"GPU 0 stats:\")\n",
    "print(f'total    : {info.total/1024.:>11.2f} MB')\n",
    "print(f'free     : {info.free/1024.:>11.2f} MB')\n",
    "print(f'used     : {info.used/1024.:>11.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be5963-8d81-4d3c-8048-caa8c4c4d587",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d03475d6-504d-48a4-bf4c-595f854548ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stackoverflow coming through to fix nvidia's broken code\n",
    "# https://stackoverflow.com/questions/59280745/inference-with-tensorrt-engine-file-on-python\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    \n",
    "class TRTModel:\n",
    "    \n",
    "    def __init__(self,engine_path, max_batch_size=1, dtype=np.float32):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self):\n",
    "        \n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        stream = cuda.Stream()\n",
    "        \n",
    "        for binding in self.engine:\n",
    "            size = trt.volume(self.engine.get_binding_shape(binding)) * self.max_batch_size\n",
    "            host_mem = cuda.pagelocked_empty(size, self.dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            \n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        \n",
    "        return inputs, outputs, bindings, stream\n",
    "       \n",
    "            \n",
    "    def __call__(self,x:np.ndarray, dim_1=312, dim_2=32):\n",
    "        x = x.astype(self.dtype)\n",
    "        np.copyto(self.inputs[0].host,x.ravel())\n",
    "        for inp in self.inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "        self.context.execute_async(batch_size=self.max_batch_size, bindings=self.bindings, stream_handle=self.stream.handle)\n",
    "        for out in self.outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream) \n",
    "        self.stream.synchronize()\n",
    "        return [out.host.reshape(self.max_batch_size, dim_1, dim_2) for out in self.outputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd181e1b-eb15-44e1-ab90-7f5cd54664c2",
   "metadata": {},
   "source": [
    "### FP32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90b42a86-78b9-4d24-afc5-7fd0f0f4fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 'wav2vec2-ctc.trt'\n",
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"loading model '{tensorrt_file_name}'\")\n",
    "speech = np.copy(speech_input).astype(np.float32)\n",
    "model = TRTModel(tensorrt_file_name)\n",
    "trt32_prediction = model(speech)[0]\n",
    "print(trt32_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbbd93b0-965d-4adf-9852-f4ae6956af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 256/256 [00:02<00:00, 95.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for TensorRT fp32 GPU inference: 10.519 ms per inference\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "for _ in tqdm.trange(test_iters):\n",
    "    out = model(speech)\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "trt32_inference = e - s\n",
    "print(f\"time for TensorRT fp32 GPU inference: {trt32_inference*1000./test_iters:.3f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9780dba1-24fe-4215-a80b-fcb9efeffbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92cbe753-3f24-4405-8748-f19838a35487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 stats:\n",
      "total    : 11283904.00 MB\n",
      "free     :  6576704.00 MB\n",
      "used     :  4707200.00 MB\n"
     ]
    }
   ],
   "source": [
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(\"GPU 0 stats:\")\n",
    "print(f'total    : {info.total/1024.:>11.2f} MB')\n",
    "print(f'free     : {info.free/1024.:>11.2f} MB')\n",
    "print(f'used     : {info.used/1024.:>11.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383f4c4-583d-428a-800b-1a837bd92839",
   "metadata": {},
   "source": [
    "### FP16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d73a9c3-c950-4021-b743-cf81c01e5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model 'wav2vec2-ctc.trt' as fp16\n",
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"loading model '{tensorrt_file_name}' as fp16\")\n",
    "speech_fp16 = np.copy(speech_input).astype(np.float16)\n",
    "model_fp16 = TRTModel(tensorrt_file_name, dtype=np.float16)\n",
    "trt16_prediction = model_fp16(speech_fp16)[0]\n",
    "print(trt16_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9146fe0-acdc-4a13-89f4-9e6908281f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 256/256 [00:02<00:00, 97.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for TensorRT fp16 GPU inference: 10.311 ms per inference\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "for _ in tqdm.trange(test_iters):\n",
    "    out_fp16 = out = model_fp16(speech_fp16)\n",
    "e = time.perf_counter()\n",
    "time.sleep(0.5)\n",
    "trt16_inference = e - s\n",
    "print(f\"time for TensorRT fp16 GPU inference: {trt16_inference*1000./test_iters:.3f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "98d43337-39cd-4ad3-8f38-2c005368605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 312, 32)\n",
      "(1, 312, 32)\n",
      "(1, 312, 32)\n",
      "(1, 312, 32)\n",
      "(1, 312, 32)\n"
     ]
    }
   ],
   "source": [
    "print(torch_prediction.shape)\n",
    "print(ort32_prediction.shape)\n",
    "print(ort16_prediction.shape)\n",
    "print(trt32_prediction.shape)\n",
    "print(trt16_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2259556-095e-47b5-bc99-98b8cbd34882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/numeric.py:2366: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * ones_like(cond)\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(torch_prediction, ort32_prediction, rtol=1e-03, atol=1e-05))\n",
    "print(np.allclose(torch_prediction, ort16_prediction, rtol=1e-03, atol=1e-05))\n",
    "print(np.allclose(torch_prediction, trt32_prediction, rtol=1e-03, atol=1e-05))\n",
    "print(np.allclose(torch_prediction, trt16_prediction, rtol=1e-03, atol=1e-05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6c8d6ab-8a1a-43b7-a001-45ead78c5822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.153247  , -18.570345  , -18.473131  , -18.496647  ,\n",
       "         0.7655839 ,   0.38968888,  -1.3583082 ,   0.5098935 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_prediction[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9c5028cf-2636-4c71-b6db-27983cfd6a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.153246  , -18.570337  , -18.473124  , -18.496635  ,\n",
       "         0.76558596,   0.38968766,  -1.3583101 ,   0.50989   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort32_prediction[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c53d8552-ca2a-4a12-9405-003055155dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan], dtype=float16)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort16_prediction[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5b905cdf-a819-45ff-8c46-b13a39f79f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.153352  , -18.571026  , -18.473814  , -18.497326  ,\n",
       "         0.76548517,   0.38961968,  -1.3583169 ,   0.51025254],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt32_prediction[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3a36e07-2c0b-42c7-8b60-eaf0641dda0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,  2.578e+00,  3.718e+03, -2.928e+00,  2.352e-01,\n",
       "       -2.926e+00, -4.323e-04, -2.926e+00], dtype=float16)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt16_prediction[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f366555f-e001-4b1c-8c05-29555b248f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec 2 results:\n",
      "pytorch      fp32 inference: 15.128 ms per request\n",
      "onnx-runtime fp32 inference: 14.541 ms per request\tresult is close? True\tnan issue?: False\n",
      "onnx-runtime fp16 inference:  7.733 ms per request\tresult is close? False\tnan issue?: True\n",
      "tensorRT     fp32 inference: 10.519 ms per request\tresult is close? False\tnan issue?: False\n",
      "tensorRT     fp16 inference: 10.311 ms per request\tresult is close? False\tnan issue?: True\n"
     ]
    }
   ],
   "source": [
    "print(\"wav2vec 2 results:\")\n",
    "print(f\"pytorch      fp32 inference: {torch_inference*1000./test_iters:>6.3f} ms per request\")\n",
    "print(f\"onnx-runtime fp32 inference: {ort32_inference*1000./test_iters:>6.3f} ms per request\\tresult is close? {np.allclose(torch_prediction, ort32_prediction, rtol=1e-03, atol=1e-05)}\\tnan issue?: {np.isnan(ort32_prediction).any()}\")\n",
    "print(f\"onnx-runtime fp16 inference: {ort16_inference*1000./test_iters:>6.3f} ms per request\\tresult is close? {np.allclose(torch_prediction, ort16_prediction, rtol=1e-03, atol=1e-05)}\\tnan issue?: {np.isnan(ort16_prediction).any()}\")\n",
    "print(f\"tensorRT     fp32 inference: {trt32_inference*1000./test_iters:>6.3f} ms per request\\tresult is close? {np.allclose(torch_prediction, trt32_prediction, rtol=1e-03, atol=1e-05)}\\tnan issue?: {np.isnan(trt32_prediction).any()}\")\n",
    "print(f\"tensorRT     fp16 inference: {trt16_inference*1000./test_iters:>6.3f} ms per request\\tresult is close? {np.allclose(torch_prediction, trt16_prediction, rtol=1e-03, atol=1e-05)}\\tnan issue?: {np.isnan(trt16_prediction).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "00dc8ae4-2d3c-4506-aa8f-2685bf353e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.880222104706503"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15.128-14.541)/15.128*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66866ebf-0050-4b36-bf7b-1b7d7c8ac5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
